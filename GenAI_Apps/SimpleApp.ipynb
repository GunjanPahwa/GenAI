{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebd120ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "#LangSmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40902614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7443434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data Ingestion-From the website we need to scrap the data\n",
    "import requests\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0994010",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=WebBaseLoader(\"https://docs.langchain.com/langsmith/optimize-classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe044a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='Optimize a classifier - Docs by LangChainSkip to main contentDocs by LangChain home pageLangSmithSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationTutorialsOptimize a classifierGet startedObservabilityEvaluationPrompt engineeringDeploymentPlatform setupReferenceOverviewQuickstartConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyPrompt template formatConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOn this pageThe objectiveGetting startedSet up automationsUpdate the applicationSemantic search over examplesTutorialsOptimize a classifierCopy pageCopy pageThis tutorial walks through optimizing a classifier based on user a feedback. Classifiers are great to optimize because its generally pretty simple to collect the desired output, which makes it easy to create few shot examples based on user feedback. That is exactly what we will do in this example.\\n\\u200bThe objective\\nIn this example, we will build a bot that classify GitHub issues based on their title. It will take in a title and classify it into one of many different classes. Then, we will start to collect user feedback and use that to shape how this classifier performs.\\n\\u200bGetting started\\nTo get started, we will first set it up so that we send all traces to a specific project. We can do this by setting an environment variable:\\nCopyimport os\\nos.environ[\"LANGSMITH_PROJECT\"] = \"classifier\"\\n\\nWe can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.\\nCopyimport openai\\nfrom langsmith import traceable, Client\\nimport uuid\\n\\nclient = openai.Client()\\n\\navailable_topics = [\\n    \"bug\",\\n    \"improvement\",\\n    \"new_feature\",\\n    \"documentation\",\\n    \"integration\",\\n]\\n\\nprompt_template = \"\"\"Classify the type of the issue as one of {topics}.\\nIssue: {text}\"\"\"\\n\\n@traceable(\\n    run_type=\"chain\",\\n    name=\"Classifier\",\\n)\\ndef topic_classifier(\\n    topic: str):\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": prompt_template.format(\\n                    topics=\\',\\'.join(available_topics),\\n                    text=topic,\\n                )\\n            }\\n        ],\\n    ).choices[0].message.content\\n\\nWe can then start to interact with it. When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function. We do this so we can attach feedback later on.\\nHere’s how we can invoke the application:\\nCopyfrom langsmith import uuid7\\n\\nrun_id = uuid7()\\ntopic_classifier(\\n    \"fix bug in LCEL\",\\n    langsmith_extra={\"run_id\": run_id})\\n\\nHere’s how we can attach feedback after. We can collect feedback in two forms.\\nFirst, we can collect “positive” feedback - this is for examples that the model got right.\\nCopyls_client = Client()\\nrun_id = uuid7()\\ntopic_classifier(\\n    \"fix bug in LCEL\",\\n    langsmith_extra={\"run_id\": run_id})\\nls_client.create_feedback(\\n    run_id,\\n    key=\"user-score\",\\n    score=1.0,\\n)\\n\\nNext, we can focus on collecting feedback that corresponds to a “correction” to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.\\nCopyls_client = Client()\\nrun_id = uuid7()\\ntopic_classifier(\\n    \"fix bug in documentation\",\\n    langsmith_extra={\"run_id\": run_id})\\nls_client.create_feedback(\\n    run_id,\\n    key=\"correction\",\\n    correction=\"documentation\")\\n\\n\\u200bSet up automations\\nWe can now set up automations to move examples with feedback of some form into a dataset. We will set up two automations, one for positive feedback and the other for negative feedback.\\nThe first will take all runs with positive feedback and automatically add them to a dataset. The logic behind this is that any run with positive feedback we can use as a good example in future iterations. Let’s create a dataset called classifier-github-issues to add this data to.\\n\\nThe second will take all runs with a correction and use a webhook to add them to a dataset. When creating this webhook, we will select the option to “Use Corrections”. This option will make it so that when creating a dataset from a run, rather than using the output of the run as the gold-truth output of the datapoint, it will use the correction.\\n\\n\\u200bUpdate the application\\nWe can now update our code to pull down the dataset we are sending runs to. Once we pull it down, we can create a string with the examples in it. We can then put this string as part of the prompt!\\nCopy### NEW CODE ###\\n# Initialize the LangSmith Client so we can use to get the dataset\\nls_client = Client()\\n\\n# Create a function that will take in a list of examples and format them into a string\\ndef create_example_string(examples):\\n    final_strings = []\\n    for e in examples:\\n        final_strings.append(f\"Input: {e.inputs[\\'topic\\']}\\\\n> {e.outputs[\\'output\\']}\")\\n    return \"\\\\n\\\\n\".join(final_strings)\\n### NEW CODE ###\\n\\nclient = openai.Client()\\n\\navailable_topics = [\\n    \"bug\",\\n    \"improvement\",\\n    \"new_feature\",\\n    \"documentation\",\\n    \"integration\",\\n]\\n\\nprompt_template = \"\"\"Classify the type of the issue as one of {topics}.\\n\\nHere are some examples:\\n{examples}\\n\\nBegin!\\nIssue: {text}\\n>\"\"\"\\n\\n@traceable(\\n    run_type=\"chain\",\\n    name=\"Classifier\",\\n)\\ndef topic_classifier(\\n    topic: str):\\n    # We can now pull down the examples from the dataset\\n    # We do this inside the function so it always get the most up-to-date examples,\\n    # But this can be done outside and cached for speed if desired\\n    examples = list(ls_client.list_examples(dataset_name=\"classifier-github-issues\"))  # <- New Code\\n    example_string = create_example_string(examples)\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": prompt_template.format(\\n                    topics=\\',\\'.join(available_topics),\\n                    text=topic,\\n                    examples=example_string,\\n                )\\n            }\\n        ],\\n    ).choices[0].message.content\\n\\nIf now run the application with a similar input as before, we can see that it correctly learns that anything related to docs (even if a bug) should be classified as documentation\\nCopyls_client = Client()\\nrun_id = uuid7()\\ntopic_classifier(\\n    \"address bug in documentation\",\\n    langsmith_extra={\"run_id\": run_id})\\n\\n\\u200bSemantic search over examples\\nOne additional thing we can do is only use the most semantically similar examples. This is useful when you start to build up a lot of examples.\\nIn order to do this, we can first define an example to find the k most similar examples:\\nCopyimport numpy as np\\n\\ndef find_similar(examples, topic, k=5):\\n    inputs = [e.inputs[\\'topic\\'] for e in examples] + [topic]\\n    vectors = client.embeddings.create(input=inputs, model=\"text-embedding-3-small\")\\n    vectors = [e.embedding for e in vectors.data]\\n    vectors = np.array(vectors)\\n    args = np.argsort(-vectors.dot(vectors[-1])[:-1])[:5]\\n    examples = [examples[i] for i in args]\\n    return examples\\n\\nWe can then use that in the application\\nCopyls_client = Client()\\n\\ndef create_example_string(examples):\\n    final_strings = []\\n    for e in examples:\\n        final_strings.append(f\"Input: {e.inputs[\\'topic\\']}\\\\n> {e.outputs[\\'output\\']}\")\\n    return \"\\\\n\\\\n\".join(final_strings)\\n\\nclient = openai.Client()\\n\\navailable_topics = [\\n    \"bug\",\\n    \"improvement\",\\n    \"new_feature\",\\n    \"documentation\",\\n    \"integration\",\\n]\\n\\nprompt_template = \"\"\"Classify the type of the issue as one of {topics}.\\n\\nHere are some examples:\\n{examples}\\n\\nBegin!\\nIssue: {text}\\n>\"\"\"\\n\\n@traceable(\\n    run_type=\"chain\",\\n    name=\"Classifier\",\\n)\\ndef topic_classifier(\\n    topic: str):\\n    examples = list(ls_client.list_examples(dataset_name=\"classifier-github-issues\"))\\n    examples = find_similar(examples, topic)\\n    example_string = create_example_string(examples)\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": prompt_template.format(\\n                    topics=\\',\\'.join(available_topics),\\n                    text=topic,\\n                    examples=example_string,\\n                )\\n            }\\n        ],\\n    ).choices[0].message.content\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoConnect to a custom modelPreviousHow to sync prompts with GitHubNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by\\n')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a257dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing into chunks \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "936b2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9954f137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='Optimize a classifier - Docs by LangChainSkip to main contentDocs by LangChain home pageLangSmithSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationTutorialsOptimize a classifierGet startedObservabilityEvaluationPrompt engineeringDeploymentPlatform setupReferenceOverviewQuickstartConceptsCreate and update promptsCreate a promptManage promptsManage prompts programmaticallyPrompt template formatConfigure prompt settingsUse tools in a promptInclude multimodal content in a promptWrite your prompt with AIConnect to modelsTutorialsOptimize a classifierSync prompts with GitHubTest multi-turn conversationsOn this pageThe objectiveGetting startedSet up automationsUpdate the applicationSemantic search over examplesTutorialsOptimize a classifierCopy pageCopy pageThis tutorial walks through optimizing a classifier based on user a feedback. Classifiers are great to optimize because its generally pretty simple to collect the desired output, which makes it easy to create few shot'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='through optimizing a classifier based on user a feedback. Classifiers are great to optimize because its generally pretty simple to collect the desired output, which makes it easy to create few shot examples based on user feedback. That is exactly what we will do in this example.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='\\u200bThe objective\\nIn this example, we will build a bot that classify GitHub issues based on their title. It will take in a title and classify it into one of many different classes. Then, we will start to collect user feedback and use that to shape how this classifier performs.\\n\\u200bGetting started\\nTo get started, we will first set it up so that we send all traces to a specific project. We can do this by setting an environment variable:\\nCopyimport os\\nos.environ[\"LANGSMITH_PROJECT\"] = \"classifier\"'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='We can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.\\nCopyimport openai\\nfrom langsmith import traceable, Client\\nimport uuid\\n\\nclient = openai.Client()\\n\\navailable_topics = [\\n    \"bug\",\\n    \"improvement\",\\n    \"new_feature\",\\n    \"documentation\",\\n    \"integration\",\\n]\\n\\nprompt_template = \"\"\"Classify the type of the issue as one of {topics}.\\nIssue: {text}\"\"\"\\n\\n@traceable(\\n    run_type=\"chain\",\\n    name=\"Classifier\",\\n)\\ndef topic_classifier(\\n    topic: str):\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": prompt_template.format(\\n                    topics=\\',\\'.join(available_topics),\\n                    text=topic,\\n                )\\n            }\\n        ],\\n    ).choices[0].message.content'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='We can then start to interact with it. When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function. We do this so we can attach feedback later on.\\nHere’s how we can invoke the application:\\nCopyfrom langsmith import uuid7\\n\\nrun_id = uuid7()\\ntopic_classifier(\\n    \"fix bug in LCEL\",\\n    langsmith_extra={\"run_id\": run_id})\\n\\nHere’s how we can attach feedback after. We can collect feedback in two forms.\\nFirst, we can collect “positive” feedback - this is for examples that the model got right.\\nCopyls_client = Client()\\nrun_id = uuid7()\\ntopic_classifier(\\n    \"fix bug in LCEL\",\\n    langsmith_extra={\"run_id\": run_id})\\nls_client.create_feedback(\\n    run_id,\\n    key=\"user-score\",\\n    score=1.0,\\n)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='Next, we can focus on collecting feedback that corresponds to a “correction” to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.\\nCopyls_client = Client()\\nrun_id = uuid7()\\ntopic_classifier(\\n    \"fix bug in documentation\",\\n    langsmith_extra={\"run_id\": run_id})\\nls_client.create_feedback(\\n    run_id,\\n    key=\"correction\",\\n    correction=\"documentation\")\\n\\n\\u200bSet up automations\\nWe can now set up automations to move examples with feedback of some form into a dataset. We will set up two automations, one for positive feedback and the other for negative feedback.\\nThe first will take all runs with positive feedback and automatically add them to a dataset. The logic behind this is that any run with positive feedback we can use as a good example in future iterations. Let’s create a dataset called classifier-github-issues to add this data to.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='The second will take all runs with a correction and use a webhook to add them to a dataset. When creating this webhook, we will select the option to “Use Corrections”. This option will make it so that when creating a dataset from a run, rather than using the output of the run as the gold-truth output of the datapoint, it will use the correction.\\n\\n\\u200bUpdate the application\\nWe can now update our code to pull down the dataset we are sending runs to. Once we pull it down, we can create a string with the examples in it. We can then put this string as part of the prompt!\\nCopy### NEW CODE ###\\n# Initialize the LangSmith Client so we can use to get the dataset\\nls_client = Client()\\n\\n# Create a function that will take in a list of examples and format them into a string\\ndef create_example_string(examples):\\n    final_strings = []\\n    for e in examples:\\n        final_strings.append(f\"Input: {e.inputs[\\'topic\\']}\\\\n> {e.outputs[\\'output\\']}\")\\n    return \"\\\\n\\\\n\".join(final_strings)\\n### NEW CODE ###'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='client = openai.Client()\\n\\navailable_topics = [\\n    \"bug\",\\n    \"improvement\",\\n    \"new_feature\",\\n    \"documentation\",\\n    \"integration\",\\n]\\n\\nprompt_template = \"\"\"Classify the type of the issue as one of {topics}.\\n\\nHere are some examples:\\n{examples}\\n\\nBegin!\\nIssue: {text}\\n>\"\"\"'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='Here are some examples:\\n{examples}\\n\\nBegin!\\nIssue: {text}\\n>\"\"\"\\n\\n@traceable(\\n    run_type=\"chain\",\\n    name=\"Classifier\",\\n)\\ndef topic_classifier(\\n    topic: str):\\n    # We can now pull down the examples from the dataset\\n    # We do this inside the function so it always get the most up-to-date examples,\\n    # But this can be done outside and cached for speed if desired\\n    examples = list(ls_client.list_examples(dataset_name=\"classifier-github-issues\"))  # <- New Code\\n    example_string = create_example_string(examples)\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": prompt_template.format(\\n                    topics=\\',\\'.join(available_topics),\\n                    text=topic,\\n                    examples=example_string,\\n                )\\n            }\\n        ],\\n    ).choices[0].message.content'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='If now run the application with a similar input as before, we can see that it correctly learns that anything related to docs (even if a bug) should be classified as documentation\\nCopyls_client = Client()\\nrun_id = uuid7()\\ntopic_classifier(\\n    \"address bug in documentation\",\\n    langsmith_extra={\"run_id\": run_id})\\n\\n\\u200bSemantic search over examples\\nOne additional thing we can do is only use the most semantically similar examples. This is useful when you start to build up a lot of examples.\\nIn order to do this, we can first define an example to find the k most similar examples:\\nCopyimport numpy as np\\n\\ndef find_similar(examples, topic, k=5):\\n    inputs = [e.inputs[\\'topic\\'] for e in examples] + [topic]\\n    vectors = client.embeddings.create(input=inputs, model=\"text-embedding-3-small\")\\n    vectors = [e.embedding for e in vectors.data]\\n    vectors = np.array(vectors)\\n    args = np.argsort(-vectors.dot(vectors[-1])[:-1])[:5]\\n    examples = [examples[i] for i in args]\\n    return examples'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='We can then use that in the application\\nCopyls_client = Client()\\n\\ndef create_example_string(examples):\\n    final_strings = []\\n    for e in examples:\\n        final_strings.append(f\"Input: {e.inputs[\\'topic\\']}\\\\n> {e.outputs[\\'output\\']}\")\\n    return \"\\\\n\\\\n\".join(final_strings)\\n\\nclient = openai.Client()\\n\\navailable_topics = [\\n    \"bug\",\\n    \"improvement\",\\n    \"new_feature\",\\n    \"documentation\",\\n    \"integration\",\\n]\\n\\nprompt_template = \"\"\"Classify the type of the issue as one of {topics}.\\n\\nHere are some examples:\\n{examples}\\n\\nBegin!\\nIssue: {text}\\n>\"\"\"'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='prompt_template = \"\"\"Classify the type of the issue as one of {topics}.\\n\\nHere are some examples:\\n{examples}\\n\\nBegin!\\nIssue: {text}\\n>\"\"\"\\n\\n@traceable(\\n    run_type=\"chain\",\\n    name=\"Classifier\",\\n)\\ndef topic_classifier(\\n    topic: str):\\n    examples = list(ls_client.list_examples(dataset_name=\"classifier-github-issues\"))\\n    examples = find_similar(examples, topic)\\n    example_string = create_example_string(examples)\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": prompt_template.format(\\n                    topics=\\',\\'.join(available_topics),\\n                    text=topic,\\n                    examples=example_string,\\n                )\\n            }\\n        ],\\n    ).choices[0].message.content'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='Edit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoConnect to a custom modelPreviousHow to sync prompts with GitHubNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08bccc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting this text to vectors\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2986f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use FAISS Database\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb=FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c04e0d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x27f11721b70>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed1d0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query from a vectorstore db\n",
    "query=\"we will build a bot that classify GitHub issues\"\n",
    "result=vectorstoredb.similarity_search(query) #this will try to give you based on context all the info that is available near those vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a469d532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\u200bThe objective\\nIn this example, we will build a bot that classify GitHub issues based on their title. It will take in a title and classify it into one of many different classes. Then, we will start to collect user feedback and use that to shape how this classifier performs.\\n\\u200bGetting started\\nTo get started, we will first set it up so that we send all traces to a specific project. We can do this by setting an environment variable:\\nCopyimport os\\nos.environ[\"LANGSMITH_PROJECT\"] = \"classifier\"'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d86de77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is i need to ask a much more meaningful question\n",
    "#I really need to provide context with respect to that particular question.\n",
    "#we use retrieval chain for this \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template( #the context is the information that i will be giving my LLM model regarding my document or text\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context> \n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\n",
    "\"\"\"\n",
    ") #example say i give a particular text, now i need to get context information\n",
    "#context information basically means that instead of searching the entire page, i can give a paragraph and say that go and search for this particular text in this paragraph\n",
    "#so paragraph becomes my context and due to this my search will also happen quickly.\n",
    "#But how do i get my paragraph?\n",
    "#For this we will be using document chain\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d349cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "628d9480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context, the completion of the sentence is: \"...based on their title.\" \\n\\nSo, the full sentence is: \"We will build a bot that classify GitHub issues based on their title.\"'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"we will build a bot that classify GitHub issues\",\n",
    "    \"context\":[Document(page_content=\"we will build a bot that classify GitHub issues based on their title. It will take in a title and classify it into one of many different classes.\")] #Adding context manually\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4208259",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we just set up. That way, we can use the retriever to dynamically select the most relevant documents and pass those in for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c31cbee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x27f11721b70>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input--->Retriever--->can be considered as an interface--->vectorstoredb\n",
    "#When we create vectorstoredb we convert it to a retriever, whihc is an interface wrt any input, so i can pass the input to retriever and get the output from vectorstore\n",
    "\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d466b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstoredb.as_retriever() #retriever already knows how to fetch context, we dont have to fill it manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c0f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain) #we dont pass context because the retriever generates it dynamically for every query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0541de7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000027F11721B70>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context> \\n{context}\\n</context>\\nQuestion: {input}\\n'), additional_kwargs={})])\n",
       "            | ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x0000027F11721960>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000027F11720FA0>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser()\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7189b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "##get the response from the llm\n",
    "response=retrieval_chain.invoke({\"input\":\"we will build a bot that classify GitHub issues\"})\n",
    "#Langchain automatically injects retrieved docs into {context}, injects user query into {input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "751c2915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's correct. According to the provided context, the objective is to build a bot that classifies GitHub issues based on their title, categorizing them into one of many different classes.\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "574aa1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'we will build a bot that classify GitHub issues',\n",
       " 'context': [Document(id='99aadc5b-b331-4d0d-a217-d8365ece5122', metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='\\u200bThe objective\\nIn this example, we will build a bot that classify GitHub issues based on their title. It will take in a title and classify it into one of many different classes. Then, we will start to collect user feedback and use that to shape how this classifier performs.\\n\\u200bGetting started\\nTo get started, we will first set it up so that we send all traces to a specific project. We can do this by setting an environment variable:\\nCopyimport os\\nos.environ[\"LANGSMITH_PROJECT\"] = \"classifier\"'),\n",
       "  Document(id='009ae338-8aa8-49a6-bb5e-97c9a415e9e3', metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='We can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.\\nCopyimport openai\\nfrom langsmith import traceable, Client\\nimport uuid\\n\\nclient = openai.Client()\\n\\navailable_topics = [\\n    \"bug\",\\n    \"improvement\",\\n    \"new_feature\",\\n    \"documentation\",\\n    \"integration\",\\n]\\n\\nprompt_template = \"\"\"Classify the type of the issue as one of {topics}.\\nIssue: {text}\"\"\"\\n\\n@traceable(\\n    run_type=\"chain\",\\n    name=\"Classifier\",\\n)\\ndef topic_classifier(\\n    topic: str):\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": prompt_template.format(\\n                    topics=\\',\\'.join(available_topics),\\n                    text=topic,\\n                )\\n            }\\n        ],\\n    ).choices[0].message.content'),\n",
       "  Document(id='199bae69-6f80-4e41-b791-554fe52c3d47', metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='Next, we can focus on collecting feedback that corresponds to a “correction” to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.\\nCopyls_client = Client()\\nrun_id = uuid7()\\ntopic_classifier(\\n    \"fix bug in documentation\",\\n    langsmith_extra={\"run_id\": run_id})\\nls_client.create_feedback(\\n    run_id,\\n    key=\"correction\",\\n    correction=\"documentation\")\\n\\n\\u200bSet up automations\\nWe can now set up automations to move examples with feedback of some form into a dataset. We will set up two automations, one for positive feedback and the other for negative feedback.\\nThe first will take all runs with positive feedback and automatically add them to a dataset. The logic behind this is that any run with positive feedback we can use as a good example in future iterations. Let’s create a dataset called classifier-github-issues to add this data to.'),\n",
       "  Document(id='7c573414-29d8-4c20-8016-9107e5505234', metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='Edit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoConnect to a custom modelPreviousHow to sync prompts with GitHubNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')],\n",
       " 'answer': \"That's correct. According to the provided context, the objective is to build a bot that classifies GitHub issues based on their title, categorizing them into one of many different classes.\"}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01ef9e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='99aadc5b-b331-4d0d-a217-d8365ece5122', metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='\\u200bThe objective\\nIn this example, we will build a bot that classify GitHub issues based on their title. It will take in a title and classify it into one of many different classes. Then, we will start to collect user feedback and use that to shape how this classifier performs.\\n\\u200bGetting started\\nTo get started, we will first set it up so that we send all traces to a specific project. We can do this by setting an environment variable:\\nCopyimport os\\nos.environ[\"LANGSMITH_PROJECT\"] = \"classifier\"'),\n",
       " Document(id='009ae338-8aa8-49a6-bb5e-97c9a415e9e3', metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='We can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.\\nCopyimport openai\\nfrom langsmith import traceable, Client\\nimport uuid\\n\\nclient = openai.Client()\\n\\navailable_topics = [\\n    \"bug\",\\n    \"improvement\",\\n    \"new_feature\",\\n    \"documentation\",\\n    \"integration\",\\n]\\n\\nprompt_template = \"\"\"Classify the type of the issue as one of {topics}.\\nIssue: {text}\"\"\"\\n\\n@traceable(\\n    run_type=\"chain\",\\n    name=\"Classifier\",\\n)\\ndef topic_classifier(\\n    topic: str):\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        temperature=0,\\n        messages=[\\n            {\\n                \"role\": \"user\",\\n                \"content\": prompt_template.format(\\n                    topics=\\',\\'.join(available_topics),\\n                    text=topic,\\n                )\\n            }\\n        ],\\n    ).choices[0].message.content'),\n",
       " Document(id='199bae69-6f80-4e41-b791-554fe52c3d47', metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='Next, we can focus on collecting feedback that corresponds to a “correction” to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.\\nCopyls_client = Client()\\nrun_id = uuid7()\\ntopic_classifier(\\n    \"fix bug in documentation\",\\n    langsmith_extra={\"run_id\": run_id})\\nls_client.create_feedback(\\n    run_id,\\n    key=\"correction\",\\n    correction=\"documentation\")\\n\\n\\u200bSet up automations\\nWe can now set up automations to move examples with feedback of some form into a dataset. We will set up two automations, one for positive feedback and the other for negative feedback.\\nThe first will take all runs with positive feedback and automatically add them to a dataset. The logic behind this is that any run with positive feedback we can use as a good example in future iterations. Let’s create a dataset called classifier-github-issues to add this data to.'),\n",
       " Document(id='7c573414-29d8-4c20-8016-9107e5505234', metadata={'source': 'https://docs.langchain.com/langsmith/optimize-classifier', 'title': 'Optimize a classifier - Docs by LangChain', 'language': 'en'}, page_content='Edit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoConnect to a custom modelPreviousHow to sync prompts with GitHubNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context'] #print context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17967a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dfbbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c5ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e91ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
